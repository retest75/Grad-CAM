{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Hook\n",
    "\n",
    "1. In PyTorch, there are a lot of hook functon.\n",
    "2. For example, after using .backward(), PyTorch will autoderivative and backpropagation to find gradient of leaf nodes.\n",
    "3. after backpropagation, the computation graph will be destroied\n",
    "4. However, we can register(註冊) a hook to retain the gradient of middle nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoderivative and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x join gradient descent: True\n",
      "y join gradient descent: True\n",
      "x = 2.0, y = 4.0, z = 64.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x **2 # y = 4\n",
    "z = y **3 # z = 64\n",
    "\n",
    "print(f\"x join gradient descent: {x.requires_grad}\")\n",
    "print(f\"y join gradient descent: {y.requires_grad}\")\n",
    "print(f\"x = {x}, y = {y}, z = {z}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x = 192.0\n",
      "gradient of y = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacinda3\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "z.backward() # backpropagation\n",
    "print(f\"gradient of x = {x.grad}\") # 192\n",
    "print(f\"gradient of y = {y.grad}\") # None, since y is not a leaf nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Hook Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x join gradient descent: True\n",
      "y join gradient descent: True\n",
      "x = 2.0, y = 4.0, z = 64.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x **2 # y = 4\n",
    "z = y **3 # z = 64\n",
    "\n",
    "print(f\"x join gradient descent: {x.requires_grad}\")\n",
    "print(f\"y join gradient descent: {y.requires_grad}\")\n",
    "print(f\"x = {x}, y = {y}, z = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x = 192.0\n",
      "gradient of y = None\n",
      "gradient of y = 48.0\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 1. define (hook) function\n",
    "# 2. use this function to retain gradient\n",
    "# 3. tensor.register: register function as a hook of some tensor\n",
    "##########################################\n",
    "def save_gradient(grad):\n",
    "    global gradient # global variable\n",
    "    gradient = grad\n",
    "\n",
    "hook = y.register_hook(save_gradient) # 將函數註冊為張量y的鉤子\n",
    "\n",
    "\n",
    "##########################################\n",
    "z.backward()\n",
    "\n",
    "print(f\"gradient of x = {x.grad}\")   # 192\n",
    "print(f\"gradient of y = {y.grad}\")   # None, still be destroied\n",
    "print(f\"gradient of y = {gradient}\") # but we already save gradient of y in \"gradient\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backpropagation\n",
    "1. reference: https://blog.csdn.net/comli_cn/article/details/104664494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12., 194.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[2, 4]]\n",
    "                 , requires_grad=True, dtype=torch.float)\n",
    "y = torch.zeros(1, 2)\n",
    "y[0, 0] = x[0, 0]**2 + x[0, 1]\n",
    "y[0, 1] = x[0, 0]    + x[0, 1]**3\n",
    "output = 2 * y\n",
    "\n",
    "gradient = torch.tensor([[1, 2]], requires_grad=True, dtype=torch.float)\n",
    "output.backward(gradient) # gradient is weight\n",
    "print(x.grad)\n",
    "\n",
    "############### visualize ###############\n",
    "#x = /   \\\n",
    "#    | 2 |\n",
    "#    | 4 |\n",
    "#    \\   /\n",
    "#\n",
    "#output = 2 * y = /             \\\n",
    "#                 |  2*x0^2+x1  |\n",
    "#                 | 2*(x0+a1^3) |\n",
    "#                 \\             /\n",
    "#        \n",
    "#output = 2 * y = [[2*(x0^2+x1),\n",
    "#                   2*(x0+a1^3)]]\n",
    "#d(output) / d(x) = /                                 \\\n",
    "#                   |  d(out_0)/d(x0)  d(out_0)/d(x1) |\n",
    "#                   |  d(out_1)/d(x0)  d(out_1)/d(x1) |\n",
    "#                   \\                                 /\n",
    "#x.grad = \n",
    "#            /                                 \\\n",
    "#            |  d(out_0)/d(x0)  d(out_0)/d(x1) |\n",
    "#  [2 4] *   |  d(out_1)/d(x0)  d(out_1)/d(x1) |\n",
    "#            \\                                 /\n",
    "###########################################            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
